{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helpers\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modern CNN Architectures\n",
    "\n",
    "## Loading TensorBoard Graphs for pre-built models\n",
    "\n",
    "Inside of the `prebuilt` folder, there are TensorBoard graphs exported for VGGNet, InceptionV1, and ResNet models. You will use these as guidance for creating your own layer functions. Load them up in TensorBoard by using the following command (assuming you're running this command from the `notebooks` directory:\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=prebuilt\n",
    "```\n",
    "\n",
    "Navigate to `localhost:6006` in your browser. After you click on the \"Graphs\" link, you'll be able to switch to the various reference graphs by choosing from the dropdown \"runs\" option.\n",
    "\n",
    "![](images/06/tb1.png)\n",
    "\n",
    "![](images/06/tb2.png)\n",
    "\n",
    "Below is a description of each graph:\n",
    "\n",
    "* **inception_module**: This is a graph representing a single InceptionV1 module. \n",
    "* **inceptionv1**: The entire InceptionV1 network (without the \"auxiliary training branch\")\n",
    "* **residual_stack**: A set of three bottleneck residual blocks. Blocks 1 and 2 are identical, while `Block_0` showcases two nuances of the model:\n",
    "    * Using a pooling layer to downsample the inputs, labeled as \"downsample\"\n",
    "    * Using a $1\\times1$ convolution to adjust the input channel depth to match the output, labeled as \"shortcut\"\n",
    "* **resnet_152**: The entire ResNet network (152-layer version)\n",
    "* **vgg_19**: The entire VGGNet network (19-layer version)\n",
    "\n",
    "The goal of this notebook/lab is to recreate the above graphs in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Provided Layer functions\n",
    "\n",
    "#### `conv()`\n",
    "\n",
    "Creates a 2D convolutional layer with Xavier-initialized weights. Automatically detects depth from previous layer\n",
    "\n",
    "* **Arguments**\n",
    "    * `inputs`: 4D `Tensor` with shape `[batch_size, height, width, channels]`\n",
    "    * `depth`: The number of output channels this convolution should create. Scalar number.\n",
    "    * `ksize`: 2D list of integers. The dimensions of convolutional kernel (ie. [3,3], [5,5], etc)\n",
    "    * `strides`: 2D list of integers. The strides of the convolution (defaults to `[1, 1]`)\n",
    "    * `padding`: String, accepted values `'SAME'` or `'VALID'`. The type of padding to use. Defaults to `SAME`.\n",
    "    * `bval`: Floating point number. The initial values for biases\n",
    "    * `activation_fn`: Lambda function. The activation function to use. defaults to `tf.nn.relu`\n",
    "    * `scope`: The name to use for the variable scope.\n",
    "    \n",
    "* **Returns**\n",
    "    * A 4D `Tensor` after the convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv(inputs, depth, ksize, strides=[1, 1], padding='SAME',\n",
    "         bval=0.01, activation_fn=tf.nn.relu, scope=None):\n",
    "    prev_shape = inputs.get_shape().as_list()\n",
    "    prev_depth = prev_shape[-1]\n",
    "    kshape = ksize + [prev_depth, depth]\n",
    "    strides = [1] + strides + [1]\n",
    "    fan_in = np.prod(prev_shape[1:], dtype=np.float32)\n",
    "    with tf.variable_scope(scope, 'conv_layer'):\n",
    "        xavier_stddev = tf.sqrt(tf.constant(2.0, dtype=tf.float32) / fan_in, name='xavier_stddev')\n",
    "        w = tf.Variable(tf.truncated_normal(kshape, stddev=xavier_stddev), name='kernel')\n",
    "        conv = tf.nn.conv2d(inputs, w, strides, padding, name='conv')\n",
    "        if bval:\n",
    "            b = tf.Variable(tf.constant(bval, shape=[depth]), name='bias')\n",
    "            z = tf.nn.bias_add(conv, b)\n",
    "        return z if activation_fn is None else activation_fn(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### `fully_connected()`\n",
    "\n",
    "Creates a 2D fully connected layer with Xavier-initialized weights. Automatically detects depth from previous layer.\n",
    "\n",
    "* **Arguments**\n",
    "    * `inputs`: 2D `Tensor` with shape `[batch_size, depth]`\n",
    "    * `depth`: Scalar. The number of neurons in this layer\n",
    "    * `bval`: Floating point number. The initial values for biases\n",
    "    * `activation_fn`: Lambda function. The activation function to use. defaults to `tf.nn.relu`\n",
    "    * `keep_prob`: Scalar float indicating the keep probability for dropout (if any)\n",
    "    * `scope`: The name to use for the variable scope.\n",
    "    \n",
    "* **Returns**\n",
    "    * A 2D `Tensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(inputs, depth, bval=0.01, activation_fn=tf.nn.relu, \n",
    "                          keep_prob=None, scope=None):\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    prev_shape = inputs.get_shape().as_list()\n",
    "    fan_in = prev_shape[-1]\n",
    "    with tf.variable_scope(scope, 'fully_connected'):\n",
    "        xavier_stddev = tf.sqrt(tf.constant(2.0, dtype=tf.float32) / fan_in, name='xavier_stddev')\n",
    "        w = tf.Variable(tf.truncated_normal([fan_in, depth], stddev=xavier_stddev), name='W')\n",
    "        b = tf.Variable(tf.constant(bval, shape=[depth]), name='bias')\n",
    "        z = tf.matmul(inputs, w) + b\n",
    "        a = z if activation_fn is None else activation_fn(z)\n",
    "        return a if keep_prob is None else tf.nn.dropout(a, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### `avgpool()` and `maxpool()`\n",
    "\n",
    "Performs average pooling and max pooling, respectively\n",
    "\n",
    "* **Arguments**\n",
    "    * `inputs`: 4D `Tensor` with shape `[batch_size, height, width, channels]`\n",
    "    * `ksize`: 2D list of integers. The dimensions of pooling kernel (ie. [2,2] etc)\n",
    "    * `strides`: 2D list of integers. The strides of the pooling kernel.\n",
    "    * `padding`: String, accepted values `'SAME'` or `'VALID'`. The type of padding to use. Defaults to `VALID`.\n",
    "    * `name`: The name to use for the variable scope.\n",
    "    \n",
    "* **Returns**\n",
    "    * A 4D `Tensor` after the pooling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def avgpool(inputs, ksize, strides, padding='VALID', name=None):\n",
    "    with tf.name_scope(name, 'avgpool'):\n",
    "        ksize = [1] + ksize + [1]\n",
    "        strides = [1] + strides + [1]\n",
    "        return tf.nn.avg_pool(inputs, ksize, strides, padding)\n",
    "\n",
    "    \n",
    "def maxpool(inputs, ksize, strides, padding='VALID', name=None):\n",
    "    with tf.name_scope(name, 'maxpool'):\n",
    "        ksize = [1] + ksize + [1]\n",
    "        strides = [1] + strides + [1]\n",
    "        return tf.nn.max_pool(inputs, ksize, strides, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### `flatten()`\n",
    "\n",
    "Flattens an N dimensional `Tensor` into a 2D `Tensor` (ie from shape `[batch_size, a, b, c]` to shape `[batch_size, a*b*c]`\n",
    "\n",
    "* **Arguments**\n",
    "    * `inputs`: The input `Tensor` to flatten\n",
    "    * `scope`: The name to use for the variable scope.\n",
    "    \n",
    "* **Returns**\n",
    "    * A 2D flattened `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten(inputs, name=None):\n",
    "    prev_shape = inputs.get_shape().as_list()\n",
    "    fan_in = np.prod(prev_shape[1:])\n",
    "    with tf.name_scope(name, 'flatten'):\n",
    "        return tf.reshape(inputs, [-1, fan_in])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## VGGNet\n",
    "\n",
    "[VGGNet paper on arXiv.org](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "![](images/06/vggtable.png)\n",
    "\n",
    "Use the above layer functions to recreate the 19 layer VGGNet from the above table (column E). Your model function should expect two parameter inputs:\n",
    "\n",
    "* `inputs`: a 4D tensor with dtype `float32` and shape `[batch_size, 224, 224, 3]`\n",
    "* `keep_prob`: A scalar `Tensor` with dtype `float32` representing the keep_probability for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vggnet(inputs, keep_prob):\n",
    "    a = inputs\n",
    "    with tf.name_scope('conv1'):\n",
    "        for i in range(2):\n",
    "            a = conv(a, 64, [3, 3])\n",
    "    a = maxpool(a, [2, 2], [2, 2])\n",
    "    with tf.name_scope('conv2'):\n",
    "        for i in range(2):\n",
    "            a = conv(a, 128, [3, 3])\n",
    "    a = maxpool(a, [2, 2], [2, 2])\n",
    "    with tf.name_scope('conv3'):\n",
    "        for i in range(4):\n",
    "            a = conv(a, 256, [3, 3])\n",
    "    a = maxpool(a, [2, 2], [2, 2])\n",
    "    with tf.name_scope('conv4'):\n",
    "        for i in range(4):\n",
    "            a = conv(a, 512, [3, 3])\n",
    "    a = maxpool(a, [2, 2], [2, 2])\n",
    "    with tf.name_scope('conv5'):\n",
    "        for i in range(4):\n",
    "            a = conv(a, 512, [3, 3])\n",
    "    a = maxpool(a, [2, 2], [2, 2])\n",
    "    a = flatten(a)\n",
    "    a = fully_connected_layer(a, 4096)\n",
    "    a = tf.nn.dropout(a, keep_prob)\n",
    "    a = fully_connected_layer(a, 4096)\n",
    "    a = tf.nn.dropout(a, keep_prob)\n",
    "    a = fully_connected_layer(a, 1000, activation_fn=None)\n",
    "    return tf.nn.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test module: Run once you're ready to check your work\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.random_normal([10, 224, 224, 3])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    output = vggnet(inputs, keep_prob)\n",
    "    writer = tf.summary.FileWriter('tbout/vggnet', graph=graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run TensorBoard to check your work\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=tbout/vggnet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inception-v1\n",
    "\n",
    "[Inception paper on arXiv.org](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "### Define an Inception module layer function\n",
    "\n",
    "Create a function that generates the following graph (from the paper):\n",
    "\n",
    "![](images/06/inception2.png)\n",
    "\n",
    "Your function should have the following parameters:\n",
    "\n",
    "* `inputs`: 4D `Tensor` inputs, with shape `[batch_size, height, width, channels]` \n",
    "* `depth_1x1`: Scalar int. The number of output channels from the `1x1` branch of the module.\n",
    "* `reduce_3x3`: Scalar int. The number of channels the `1x1` convolution on the `3x3` branch should output into the `3x3` convolution.\n",
    "* `depth_3x3`: Scalar int. The number of output channels from the `3x3` branch of the module.\n",
    "* `reduce_5x5`: Scalar int. The number of channels the `1x1` convolution on the `5x5` branch should output into the `5x5` convolution.\n",
    "* `depth_5x5`: Scalar int. The number of output channels from the `5x5` branch of the module.\n",
    "* `pool_proj`: Scalar int. The number of output channels the `1x1` convolution on the max pool branch\n",
    "* `scope`: Optional string. A name for the module. (Don't need to worry about this for implementation)\n",
    "\n",
    "Here are some additional hints about creating the module:\n",
    "\n",
    "* The max pooling operation needs to be `3x3` with stride 1\n",
    "* The above function parameters follows Table 1 from the paper (see below for embedded table in notebook) \n",
    "* You'll need to use [`tf.concat()`](https://www.tensorflow.org/api_docs/python/tf/concat) to attach all of the outputs from each branch to one another. `tf.concat` takes two parameters. The first is a list of `Tensor` objects to concatenate, and the other is the axis (or dimension) they should be attached. <br> For example, if you have two tensors, `a` and `b`, with shape `[10, 2, 2, 5]`, here are the results of different concat axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis = 0: (20, 2, 2, 5)\n",
      "Axis = 1: (10, 4, 2, 5)\n",
      "Axis = 2: (10, 2, 4, 5)\n",
      "Axis = 3: (10, 2, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros([10, 2, 2, 5])\n",
    "b = tf.ones([10, 2, 2, 5])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('Axis = 0: {}'.format(tf.concat([a, b], 0).get_shape()))\n",
    "    print('Axis = 1: {}'.format(tf.concat([a, b], 1).get_shape()))\n",
    "    print('Axis = 2: {}'.format(tf.concat([a, b], 2).get_shape()))\n",
    "    print('Axis = 3: {}'.format(tf.concat([a, b], 3).get_shape()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's your job to determine which axis is appropriate for the goal of concatenating the filters of each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inception_module(inputs, depth_1x1, reduce_3x3, depth_3x3, reduce_5x5,\n",
    "                     depth_5x5, pool_proj, scope=None):\n",
    "    with tf.variable_scope(scope, 'inception_module') as scope:\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            # 1x1 convolution path\n",
    "            relu_0 = conv(inputs=inputs,\n",
    "                          ksize=[1, 1],\n",
    "                          depth=depth_1x1,\n",
    "                          strides=[1, 1],\n",
    "                          padding='SAME',\n",
    "                          scope='conv_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            # 3x3 convolution path\n",
    "            relu_1 = conv(inputs=inputs,\n",
    "                          ksize=[1, 1],\n",
    "                          depth=reduce_3x3,\n",
    "                          strides=[1, 1],\n",
    "                          padding='SAME',\n",
    "                          scope='conv_1x1')\n",
    "            relu_1 = conv(inputs=relu_1,\n",
    "                          ksize=[3, 3],\n",
    "                          depth=depth_3x3,\n",
    "                          strides=[1, 1],\n",
    "                          padding='SAME',\n",
    "                          scope='conv_3x3')\n",
    "        with tf.variable_scope('Branch_2',):\n",
    "            # 5x5 convolution path\n",
    "            relu_2 = conv(inputs=inputs,\n",
    "                          ksize=[1, 1],\n",
    "                          depth=reduce_5x5,\n",
    "                          strides=[1, 1],\n",
    "                          padding='SAME',\n",
    "                          scope='conv_1x1')\n",
    "            relu_2 = conv(inputs=relu_2,\n",
    "                          ksize=[5, 5],\n",
    "                          depth=depth_5x5,\n",
    "                          strides=[1, 1],\n",
    "                          padding='SAME',\n",
    "                          scope='conv_5x5')\n",
    "        with tf.variable_scope('Branch_3'):\n",
    "            pool = maxpool(inputs=inputs,\n",
    "                           ksize=[3, 3],\n",
    "                           strides=[1, 1],\n",
    "                           padding='SAME',\n",
    "                           name='maxpool')\n",
    "            relu_3 = conv(inputs=pool,\n",
    "                          ksize=[1, 1],\n",
    "                          depth=pool_proj,\n",
    "                          strides=[1, 1],\n",
    "                          padding='SAME',\n",
    "                          scope='conv_1x1')\n",
    "        return tf.concat([relu_0, relu_1, relu_2, relu_3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test module: Run once you're ready to check your work\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.random_normal([10, 28, 28, 192])\n",
    "    output = inception_module(inputs, 64, 96, 128, 16, 32, 32, 'inception_module')\n",
    "    writer = tf.summary.FileWriter('tbout/inception_module', graph=graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run TensorBoard to check your work\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=tbout/inception_module\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating the full Inception-v1 network\n",
    "\n",
    "![](images/06/inceptiontable.png)\n",
    "\n",
    "Once your Inception module is functioning correctly, try to create the inception module described in the above model. Remember that the inception_module function parameters are designed to line up with the \"#1x1\", \"#3x3 reduce\", etc columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inceptionv1(inputs, keep_prob):\n",
    "    with tf.variable_scope('inceptionv1'):\n",
    "        c = conv(inputs, 64, [7, 7], [2, 2], scope='conv_7x7_stride_2')\n",
    "        p = maxpool(c, [3, 3], [2, 2], name='maxpool')\n",
    "        c = conv(p, 192, [3, 3], scope='conv_3x3')\n",
    "        p = maxpool(c, [3, 3], [2, 2], 'SAME', name='maxpool')                   \n",
    "        i_3a = inception_module(inputs=p,\n",
    "                                depth_1x1=64,\n",
    "                                reduce_3x3=96,\n",
    "                                depth_3x3=128,\n",
    "                                reduce_5x5=16,\n",
    "                                depth_5x5=32,\n",
    "                                pool_proj=32,\n",
    "                                scope='Mixed_3a')\n",
    "        i_3b = inception_module(inputs=i_3a,\n",
    "                                depth_1x1=128,\n",
    "                                reduce_3x3=128,\n",
    "                                depth_3x3=192,\n",
    "                                reduce_5x5=32,\n",
    "                                depth_5x5=96,\n",
    "                                pool_proj=64,\n",
    "                                scope='Mixed_3b')\n",
    "        p = maxpool(i_3b, [3, 3], [2, 2], 'SAME', name='maxpool')\n",
    "        i_4a = inception_module(inputs=p,\n",
    "                                depth_1x1=192,\n",
    "                                reduce_3x3=96,\n",
    "                                depth_3x3=208,\n",
    "                                reduce_5x5=16,\n",
    "                                depth_5x5=48,\n",
    "                                pool_proj=64,\n",
    "                                scope='Mixed_4a')\n",
    "        i_4b = inception_module(inputs=i_4a,\n",
    "                                depth_1x1=160,\n",
    "                                reduce_3x3=112,\n",
    "                                depth_3x3=224,\n",
    "                                reduce_5x5=24,\n",
    "                                depth_5x5=64,\n",
    "                                pool_proj=64,\n",
    "                                scope='Mixed_4b')\n",
    "        i_4c = inception_module(inputs=i_4b,\n",
    "                                depth_1x1=128,\n",
    "                                reduce_3x3=128,\n",
    "                                depth_3x3=256,\n",
    "                                reduce_5x5=24,\n",
    "                                depth_5x5=64,\n",
    "                                pool_proj=64,\n",
    "                                scope='Mixed_4c')\n",
    "        i_4d = inception_module(inputs=i_4c,\n",
    "                                depth_1x1=112,\n",
    "                                reduce_3x3=144,\n",
    "                                depth_3x3=288,\n",
    "                                reduce_5x5=32,\n",
    "                                depth_5x5=64,\n",
    "                                pool_proj=64,\n",
    "                                scope='Mixed_4d')\n",
    "        i_4e = inception_module(inputs=i_4d,\n",
    "                                depth_1x1=256,\n",
    "                                reduce_3x3=160,\n",
    "                                depth_3x3=320,\n",
    "                                reduce_5x5=32,\n",
    "                                depth_5x5=128,\n",
    "                                pool_proj=128,\n",
    "                                scope='Mixed_4e')\n",
    "        p = maxpool(i_4e, [3, 3], [2, 2], 'SAME', name='maxpool')\n",
    "        i_5a = inception_module(inputs=p,\n",
    "                                depth_1x1=256,\n",
    "                                reduce_3x3=160,\n",
    "                                depth_3x3=320,\n",
    "                                reduce_5x5=32,\n",
    "                                depth_5x5=128,\n",
    "                                pool_proj=128,\n",
    "                                scope='Mixed_5a')\n",
    "        i_5b = inception_module(inputs=i_5a,\n",
    "                                depth_1x1=384,\n",
    "                                reduce_3x3=192,\n",
    "                                depth_3x3=384,\n",
    "                                reduce_5x5=48,\n",
    "                                depth_5x5=128,\n",
    "                                pool_proj=128,\n",
    "                                scope='Mixed_5b')\n",
    "        p = avgpool(i_5b, [7, 7], [1, 1], 'VALID', name='avgpool')\n",
    "        dropped = tf.nn.dropout(p, keep_prob=keep_prob)\n",
    "        flat = flatten(dropped)\n",
    "        fc = fully_connected_layer(flat, 1000, activation_fn=None, scope='linear')\n",
    "        softmax = tf.nn.softmax(fc)\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test module: Run once you're ready to check your work\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.random_normal([10, 224, 224, 3])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    output = inceptionv1(inputs, keep_prob)\n",
    "    writer = tf.summary.FileWriter('tbout/inceptionv1', graph=graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run TensorBoard to check your work\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=tbout/inceptionv1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ResNet\n",
    "\n",
    "[ResNet paper on arXiv.org](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "### Define a residual block layer function\n",
    "\n",
    "Create a function that recreates a \"bottleneck\" residual block:\n",
    "\n",
    "![](images/06/resnet4.png)\n",
    "\n",
    "Your function, `residual_block` should take the following arguments:\n",
    "\n",
    "* `inputs`: A 4D Tensor with shape `[batch_size, height, width, channels]`\n",
    "* `bottleneck_depth`: Scalar int. Number of channels the first `1x1` and `3x3` convolutions should output. (e.g. the number 64 in the image above)\n",
    "* `output_depth`: Scalar int. Number of channels the final `1x1` convolution should output (e.g. the number 256 in the image above)\n",
    "* `downsample`: Boolean, defaults to False. If True, the function should add a maxpool (2x2, stride 2) operation before the first `1x1` convolution\n",
    "* `scope`: Optional string. Name for the residual block.\n",
    "\n",
    "Notes for implementing the residual block:\n",
    "\n",
    "* **Important**: Your function must check to see if the `output_depth` matches the number of channels in `inputs`. If it does not, the \"skip connection\" needs to have a `1x1` convolution applied to it to adjust the number of channels (i.e. `skip = conv(inputs, output_depth, [1,1],[1,1])`\n",
    "* **Important**: Don't use any activation function for the output of the final `1x1` convolution. Instead, apply the activation function _after_ you add in the skip connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def residual_block(inputs, bottleneck_depth, output_depth, downsample=False, scope=None):\n",
    "    with tf.variable_scope(scope, 'residual_block'):\n",
    "        if downsample:\n",
    "            inputs = maxpool(inputs=inputs,\n",
    "                           ksize=[2, 2],\n",
    "                           strides=[2, 2],\n",
    "                           padding='VALID',\n",
    "                           name='downsample')\n",
    "        prev_depth = inputs.get_shape()[3]\n",
    "        relu1 = conv(inputs=inputs,\n",
    "                     depth=bottleneck_depth,\n",
    "                     ksize=[1, 1],\n",
    "                     strides=[1, 1],\n",
    "                     padding='SAME',\n",
    "                     scope='conv1')\n",
    "        relu2 = conv(inputs=relu1,\n",
    "                     depth=bottleneck_depth,\n",
    "                     ksize=[3, 3],\n",
    "                     strides=[1, 1],\n",
    "                     padding='SAME',\n",
    "                     scope='conv2')\n",
    "        conv3 = conv(inputs=relu2,\n",
    "                     depth=output_depth,\n",
    "                     ksize=[1, 1],\n",
    "                     strides=[1, 1],\n",
    "                     padding='SAME',\n",
    "                     scope='conv3',\n",
    "                     activation_fn=None)\n",
    "        if inputs.get_shape() != conv3.get_shape():\n",
    "            inputs = conv(inputs=inputs,\n",
    "                        depth=output_depth,\n",
    "                        ksize=[1, 1],\n",
    "                        strides=[1, 1],\n",
    "                        padding='SAME',\n",
    "                        scope='shortcut',\n",
    "                        activation_fn=None)\n",
    "        add = tf.add(conv3, inputs)\n",
    "        return tf.nn.relu(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(tf.float32, shape=[10, 224, 224, 3], name='inputs')\n",
    "    block = inputs\n",
    "    for i in range(3):\n",
    "        ds = True if i == 0 else False  # down-sample first block only\n",
    "        block = residual_block(inputs=block,\n",
    "                               bottleneck_depth=128,\n",
    "                               output_depth=512,\n",
    "                               scope='block_{}'.format(i),\n",
    "                               downsample=ds)\n",
    "    writer = tf.summary.FileWriter('tbout/residual_stack', graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run TensorBoard to check your work\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=tbout/residual_stack\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating the full ResNet\n",
    "\n",
    "![](images/06/resnettable.png)\n",
    "\n",
    "Use your `residual_block` function to implement the 152-layer ResNet from the above table. \n",
    "\n",
    "Notes:\n",
    "\n",
    "* In between each layer section (\"conv2_x\", \"conv3_x\", etc), there is a 2x2 maxpool, stride 2.\n",
    "* Use the `for` loop in the `residual_block` test code above as a model for creating the different layer sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def resnet(inputs, keep_prob):\n",
    "    input_depth = inputs.get_shape()[3]\n",
    "    c = conv(inputs=inputs, depth=64, ksize=[7, 7], strides=[2, 2], padding='SAME', scope='conv1')\n",
    "    p = maxpool(inputs=c, ksize=[3, 3], strides=[2, 2], padding='SAME', name='maxpool_3x3')\n",
    "    block = p  # makes below loops more semantic\n",
    "    with tf.variable_scope('stack_1'):\n",
    "        for i in range(3):\n",
    "            block = residual_block(inputs=block, bottleneck_depth=64, output_depth=256, scope='block_{}'.format(i))\n",
    "    with tf.variable_scope('stack_2'):\n",
    "        for i in range(8):\n",
    "            ds = True if i == 0 else False  # down-sample first block only\n",
    "            block = residual_block(inputs=block, bottleneck_depth=128, output_depth=512, \n",
    "                                   scope='block_{}'.format(i), downsample=ds)\n",
    "    with tf.variable_scope('stack_3'):\n",
    "        for i in range(36):\n",
    "            ds = True if i == 0 else False  # down-sample first block only\n",
    "            block = residual_block(inputs=block, bottleneck_depth=256, output_depth=1024,\n",
    "                                          scope='block_{}'.format(i), downsample=ds)\n",
    "    with tf.variable_scope('stack_4'):\n",
    "        for i in range(3):\n",
    "            ds = True if i == 0 else False  # down-sample first block only\n",
    "            block = residual_block(inputs=block, bottleneck_depth=512, output_depth=2048,\n",
    "                                          scope='block_{}'.format(i), downsample=ds)\n",
    "    p = avgpool(inputs=block, ksize=[7, 7], strides=[1, 1], padding='VALID', name='avgpool_7x7')\n",
    "    flat = flatten(p)\n",
    "    fc = fully_connected_layer(flat, 1000, activation_fn=None, scope='linear')    \n",
    "    softmax = tf.nn.softmax(fc)\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test module: Run once you're ready to check your work\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.random_normal([10, 224, 224, 3])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    output = resnet(inputs, keep_prob)\n",
    "    writer = tf.summary.FileWriter('tbout/resnet', graph=graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inception-Resnet-v2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Question: are we using an activation function? It's not specified so I am assuming relu as default for now\n",
    "# except for the 1x1 convs they said had no activation - also is no bias correct - because doing batch norm?\n",
    "# Inception-Resnet-v2 stem\n",
    "def inception_stem(inputs):\n",
    "    with tf.variable_scope('stem') as scope:\n",
    "        with tf.variable_scope(\"3x3_1\"):\n",
    "            a = conv(inputs, depth=32, ksize=[3,3], strides=[2,2], padding='VALID', bval=None, activation_fn=tf.nn.relu)\n",
    "        with tf.variable_scope(\"3x3_2\"):\n",
    "            # not sure about the strides here?\n",
    "            a = conv(a, depth = 32, ksize=[3,3], strides=[2,2], padding='VALID', bval=None, activation_fn=tf.nn.relu)\n",
    "        with tf.variable_scope(\"3x3_3\"):\n",
    "            # still not sure about strides\n",
    "            a = conv(a, depth = 64, ksize=[3,3], strides=[2,2], padding='SAME', bval=None, activation_fn=tf.nn.relu)\n",
    "        with tf.variable_scope(\"3x3_maxpool_branch_1a\"):\n",
    "            aa = maxpool(a, ksize=[3,3], strides=[2,2], padding='VALID')\n",
    "        with tf.variable_scope(\"3x3_conv_branch_1b\"):\n",
    "            bb = conv(a, depth=96, ksize=[3,3], strides=[2,2], padding='VALID',bval=None, activation_fn=tf.nn.relu)\n",
    "        with tf.variable_scope(\"concat_1\"):\n",
    "            a = tf.concat([aa,bb],-1)\n",
    "        with tf.variable_scope(\"branch_2a\"):\n",
    "            # here assuming stride 1 since 1x1 conv ?? also this is not an inception block so has activation????\n",
    "            aa = conv(a, depth=64, ksize=[1,1], strides=[1,1], padding='SAME', activation_fn=tf.nn.relu)\n",
    "            # assuming stride 2 tho not sure\n",
    "            aa = conv(aa, depth=96, ksize=[3,3], strides=[2,2], padding='VALID', activation_fn=tf.nn.relu)\n",
    "        with tf.variable_scope(\"branch_2b\"):\n",
    "            bb = conv(a, depth=64, ksize=[1,1], strides=[1,1], padding='SAME', bval=None, actication_fn=tf.nn.relu)\n",
    "            bb = conv(bb, depth=64, ksize=[7,1], strides=[1,1], padding='SAME', bval=None, activation_fn=tf.nn.relu)\n",
    "            bb = conv(bb, depth=64, ksize=[1,7], strides=[1,1], padding='SAME', bval=None, activation_fn=tf.nn.relu)\n",
    "            bb = conv(bb, depth=96, ksize=[3,3], strides=[2,2], padding='VALID', bval=None, activation_fn=tf.nn.relu)\n",
    "        with tf.variable_scope(\"concat_2\"):\n",
    "            a = tf.concat([aa,bb],axis= -1)\n",
    "        with tf.variable_scope(\"branch_3a\"):\n",
    "            aa = conv(a, depth=192, ksize=[3,3],strides=[2,2], padding='VALID', bval=None, activation_fn=tf.nn.relu)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-3fb0b42abd79>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-3fb0b42abd79>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    for i in range(5)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Inception-Resnet-v2 Overall Structure\n",
    "def inception_resnet_v2_full(inputs,keep_prob):\n",
    "    input_depth - inputs.get_shape()[-1]\n",
    "    with tf.variable_scope('stem'):\n",
    "        flowing_data = inception_stem(inputs)\n",
    "    with tf.variable_scope('inception_resnet_A_x5'):\n",
    "        for i in range(5)\n",
    "            flowing_data = inception_renet_A(flowing_data)\n",
    "    with tf.variable_scope('reduction_A'):\n",
    "        flowing_data = reduction_A(flowing_data)\n",
    "    with tf.variable_scope('inception_resnet_B_x10'):\n",
    "        for i in range(10):\n",
    "            flowing_data = inception_resnet_B(flowing_data)\n",
    "    with tf.variable_scope('reduction_B'):\n",
    "        flowing_data = reduction_B(flowing_data)\n",
    "    with tf.variable_scope('inception_resnet_C_x5'):\n",
    "        for i in range(5):\n",
    "            flowing_data = inception_resnet_C(flowing_data)\n",
    "    with tf.variable_scope('avg_pool'):\n",
    "        flowing_data = avgpool(inputs, ksize, strides, padding='VALID', name=None)\n",
    "    with tf.variable_scope('dropout'):\n",
    "        flowing_data = tf.nn.dropout(flowing_data)\n",
    "    with tf.variable_scope('softmax'):\n",
    "        flowing_data = tf.nn.softmax(flowing_data)\n",
    "    return flowing_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
